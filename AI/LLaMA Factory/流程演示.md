#### 1. 训练

```
USE_MODELSCOPE_HUB=1 llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
```

**examples/train_lora/llama3_lora_sft.yaml**

```
### model

model_name_or_path: Qwen/Qwen3-0.6B-base

trust_remote_code: true

  

### method

stage: sft

do_train: true

finetuning_type: lora

lora_rank: 8

lora_target: all

  

### dataset

dataset: identity,alpaca_en_demo

template: default

cutoff_len: 2048

max_samples: 1000

overwrite_cache: true

preprocessing_num_workers: 16

dataloader_num_workers: 4

  

### output

output_dir: saves/Qwen3-0.6B/lora/sft

logging_steps: 10

save_steps: 500

plot_loss: true

overwrite_output_dir: true

save_only_model: false

report_to: none  # choices: [none, wandb, tensorboard, swanlab, mlflow]

  

### train

per_device_train_batch_size: 1

gradient_accumulation_steps: 8

learning_rate: 1.0e-4

num_train_epochs: 3.0

lr_scheduler_type: cosine

warmup_ratio: 0.1

bf16: true

ddp_timeout: 180000000

resume_from_checkpoint: null

  

### eval

# eval_dataset: alpaca_en_demo

# val_size: 0.1

# per_device_eval_batch_size: 1

# eval_strategy: steps

# eval_steps: 500
```

#### 2. 对话

```
USE_MODELSCOPE_HUB=1 llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
```

**examples/inference/llama3_lora_sft.yaml**

```
model_name_or_path: Qwen/Qwen3-0.6B-base

adapter_name_or_path: saves/Qwen3-0.6B/lora/sft

template: default

infer_backend: huggingface  # choices: [huggingface, vllm, sglang]

trust_remote_code: true
```

#### 3. 合并模型

```
USE_MODELSCOPE_HUB=1 llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
```

**examples/merge_lora/llama3_lora_sft.yaml**

```
### Note: DO NOT use quantized model or quantization_bit when merging lora adapters

  

### model

model_name_or_path: Qwen/Qwen3-0.6B-base

adapter_name_or_path: saves/Qwen3-0.6B/lora/sft

template: default

trust_remote_code: true

  

### export

export_dir: output/Qwen3-0.6B

export_size: 5

export_device: cpu  # choices: [cpu, auto]

export_legacy_format: false
```

#### 4. API访问

```
USE_MODELSCOPE_HUB=1 llamafactory-cli api examples/inference/llama3_lora_sft.yaml
```

**examples/inference/llama3_lora_sft.yaml**

```
model_name_or_path: output/Qwen3-0.6B

template: default

infer_backend: huggingface  # choices: [huggingface, vllm, sglang]

trust_remote_code: true
```

**API测试**

```
curl -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "output/Qwen3-0.6B",
    "messages": [
      {"role": "user", "content": "你是谁"}
    ],
    "max_tokens": 64,
    "temperature": 0.7
  }'
```